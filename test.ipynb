{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff74bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import xarray as xr\n",
    "import numpy as np \n",
    "import xroms\n",
    "import xarray as xr\n",
    "import pyresample \n",
    "import metpy\n",
    "import matplotlib.pyplot as plt\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd053509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function transfroming the txt file to a pandas dataframe:\n",
    "def txt_to_pd(txtfile,LAT,LON):\n",
    "    \n",
    "    # Set whcih columns to keep and set new names:\n",
    "    columns_to_keep = ['T_degC', 'T_qf', 'S', 'S_qf', 'Date', 'Time'] \n",
    "    new_column_names = ['TEMP','TEMP_QC','PSAL','PSAL_QC','Date','Time']\n",
    "    \n",
    "    # Change the names of the months to English from Norwegian\n",
    "    parse = lambda x: datetime.datetime.strptime(x.replace('Des', 'Dec').replace('Mai', 'May').replace('Okt', 'Oct'), '%d %b %Y %H:%M:%S')\n",
    "\n",
    "    #df = pd.read_csv(txtfile, delimiter='\\t', usecols=columns_to_keep, parse_dates={\"Datetime\" : ['Date', 'Time']}, date_parser = parse)\n",
    "    df = pd.read_csv(txtfile, delimiter='\\t', usecols=columns_to_keep)\n",
    "    df.columns = new_column_names\n",
    "    \n",
    "    # Combine Date and Time columns and strip any leading or trailing whitespace\n",
    "    df['TIME'] = (df['Date'] + ' ' + df['Time']).str.strip()\n",
    "    \n",
    "    # Apply the custom parsing function\n",
    "    df['TIME'] = df['TIME'].apply(parse)\n",
    "    df.drop(columns=['Date', 'Time'], inplace=True)\n",
    "    \n",
    "    # Add depth:\n",
    "    df['DEPTH'] = get_depth(txtfile)\n",
    "    df['LAT'] = LAT\n",
    "    df['LON'] = LON\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def netcdf_to_pd(txt):\n",
    "    # Load the NetCDF file using xarray\n",
    "    ds = xr.open_dataset(txt)\n",
    "\n",
    "    # Select the variables you are interested in\n",
    "    variables = ['TEMP', 'TEMP_QC', 'PSAL', 'PSAL_QC']\n",
    "\n",
    "    # Initialize an empty DataFrame to merge into\n",
    "    df_combined = pd.DataFrame()\n",
    "\n",
    "    # Loop over each variable to process and merge\n",
    "    for var in variables:\n",
    "\n",
    "        # Select the variable data\n",
    "        data = ds[var]\n",
    "    \n",
    "        # Stack the depth and time dimensions into a MultiIndex\n",
    "        stacked_data = data.stack(points=('DEPTH', 'TIME'))\n",
    "    \n",
    "    \n",
    "        # Convert the stacked DataArray to a pandas DataFrame\n",
    "        df = stacked_data.to_dataframe()\n",
    "            \n",
    "        # Temporarily rename columns to avoid conflicts\n",
    "        df.rename(columns={'DEPTH': 'Depth_col', 'TIME': 'Time_col', 'LONGITUDE': 'LON_col'}, inplace=True)\n",
    "        #        \n",
    "        df = df.reset_index()\n",
    "        df.drop(columns=['Depth_col', 'Time_col'], inplace=True)\n",
    "        #    \n",
    "        # If df_combined is empty, initialize it with the current DataFrame\n",
    "        if df_combined.empty:\n",
    "            df_combined = df\n",
    "        else:\n",
    "            # Merge the current DataFrame with the combined DataFrame\n",
    "            df_combined = pd.merge(df_combined, df, on=['DEPTH', 'TIME'], how='outer')  \n",
    "    df_combined['LON'] = ds['LONGITUDE'].values[0]\n",
    "    df_combined['LAT'] = ds['LATITUDE'].values[0]\n",
    "    return(df_combined)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc8a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(filename):\n",
    "    \n",
    "    # Use a regular expression to extract the depth value\n",
    "    depth_match = re.search(r'_(\\d+)m', filename)\n",
    "    if depth_match:\n",
    "        depth_value = int(depth_match.group(1))\n",
    "    else:\n",
    "        depth_value = None\n",
    "    return(depth_value)   \n",
    "\n",
    "def get_Zindices(ds,X,Y,target_depth): \n",
    "    \n",
    "    # ds er en xromsfil, gridfil fra norshelf\n",
    "    ds = ds.isel(xi_rho = X, eta_rho = Y)\n",
    "\n",
    "    # Extract the z_rho values at the given point\n",
    "    z_rho_values = ds.z_rho.values\n",
    "    s_rho_values = ds.s_rho.values\n",
    "\n",
    "    diff = np.abs(z_rho_values - target_depth)\n",
    "    \n",
    "    # Find the indices of the two smallest differences\n",
    "    closest_indices = diff.argsort()[0][:2]\n",
    "    \n",
    "    return(closest_indices)\n",
    "\n",
    "def find_time_index(r_time,ocean_time):\n",
    "    # r_time er datoen vi vil ha\n",
    "    days_since_1970 = (r_time - datetime.datetime(1970,1,1,0,0,0)).total_seconds()\n",
    "    ocean_time = pd.to_datetime(ocean_time)\n",
    "    # Convert ocean_time\n",
    "    ocean_time_since_1970 = (ocean_time - datetime.datetime(1970,1,1,0,0,0)).total_seconds()\n",
    "    index = np.abs(ocean_time_since_1970 - days_since_1970).argmin()\n",
    "    return(index)\n",
    "\n",
    "def get_XYpositions(filename, lons, lats):\n",
    "    \n",
    "    fh = xr.open_dataset(filename)\n",
    "    x   = np.linspace(0, fh.lat_rho.values.shape[1]-1, fh.lat_rho.values.shape[1])\n",
    "    y   = np.linspace(0, fh.lat_rho.values.shape[0]-1, fh.lat_rho.values.shape[0])\n",
    "    xi  = np.zeros_like(fh.lon_rho.values)\n",
    "    yi  = np.zeros([fh.lon_rho.values.shape[1], fh.lon_rho.values.shape[0]])\n",
    "    xi[:,:] = x\n",
    "    yi[:,:] = y\n",
    "    yi  = np.swapaxes(yi, 1, 0)\n",
    "\n",
    "    # First I define the wet points of the field as the lon,lat values with mask_rho==1 \n",
    "    sea_def = pyresample.geometry.SwathDefinition(lons= fh.lon_rho.values[np.where(fh.mask_rho)], lats = fh.lat_rho.values[np.where(fh.mask_rho)])\n",
    "\n",
    "    # Second, the full grid definiton (our target domain):\n",
    "    orig_def = pyresample.geometry.SwathDefinition(lons=lons, lats=lats)\n",
    "\n",
    "    # Then I fill the temperature field by the nearest neighbour approace.\n",
    "    # Note that only wet points are used as input. \n",
    "\n",
    "    # The radius of influence sets a limit (in meters) for how far away a true value can be from the point that will be filled\n",
    "\n",
    "    ypos = pyresample.kd_tree.resample_nearest(sea_def, yi[np.where(fh.mask_rho)], \\\n",
    "                               orig_def, radius_of_influence=2400)\n",
    "\n",
    "    xpos = pyresample.kd_tree.resample_nearest(sea_def, xi[np.where(fh.mask_rho)], \\\n",
    "                               orig_def, radius_of_influence=2400)\n",
    "    return np.array([int(x) for x in xpos]), np.array([int(y) for y in ypos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f74483d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Test-data - sjekk om det funker \\nds2=ds2.head(50)\\n# Test-data\\nds=ds.head(50)\\nprint(ds.head(5))\\nprint(\" ds2 \")\\nprint(ds2.head(5))#'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_d1 = \"/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment1/StationM_2021_hydrography.nc\"\n",
    "\n",
    "# Lese inn tekstfilene for deployment 2 (d2) ved 500, 800, 1000, 1200 m:\n",
    "\n",
    "txtfile_d2_500m  = '/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment2/StaM_SBE_20211127_500m.txt'\n",
    "txtfile_d2_800m  = '/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment2/StaM_SBE_20211127_800m.txt'\n",
    "txtfile_d2_1000m = '/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment2/StaM_SBE_20211127_1000m.txt'\n",
    "txtfile_d2_1200m = '/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment2/StaM_SBE_20211127_1200m.txt'\n",
    "txtfile_d2_2000m = '/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment2/StaM_SBE_20211127_2000m.txt'\n",
    "\n",
    "# Lese inn tekstfilene for deployment 3 (d3) ved 500, 800, 1000, 1200 m:\n",
    "\n",
    "txtfile_d3_500m  = '/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment3/StaM_SBE_20221125_500m.txt'\n",
    "txtfile_d3_800m  = '/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment3/StaM_SBE_20221125_800m.txt'\n",
    "txtfile_d3_1000m = '/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment3/StaM_SBE_20221125_1000m.txt'\n",
    "txtfile_d3_1200m = '/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment3/StaM_SBE_20221125_1200m.txt'\n",
    "txtfile_d3_2000m = '/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment3/StaM_SBE_20221125_2000m.txt'\n",
    "\n",
    "# Lese inn filer for Mooring South Cape:\n",
    "\n",
    "south_cape_df = \"/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/SouthCape/T_S_SouthCape.csv\"\n",
    "\n",
    "\n",
    "txtfile = '/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment2/StaM_SBE_20211127_1000m.txt'\n",
    "\n",
    "ds = txt_to_pd(txtfile_d2_500m,66.015,1.983)\n",
    "\n",
    "deployment1 = '/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment1/StationM_2021_hydrography.nc'\n",
    "ds2 = netcdf_to_pd(deployment1)\n",
    "\n",
    "gridfile = '/lustre/storeB/project/fou/hi/oper/norshelf/static_inputfiles/norshelf_2.4_vert_grd.nc'\n",
    "x,y = get_XYpositions(gridfile, ds2.LON.values, ds2.LAT.values)\n",
    "ds2['X'] = x\n",
    "ds2['Y'] = y\n",
    "\n",
    "x,y = get_XYpositions(gridfile, ds.LON.values, ds.LAT.values)\n",
    "ds['X'] = x\n",
    "ds['Y'] = y\n",
    "\n",
    "\n",
    "ds2['TIME'] = pd.to_datetime(ds2['TIME'])\n",
    "# Round the TIME column to the nearest hour\n",
    "ds2['TIME'] = ds2['TIME'].dt.round('H')\n",
    "\n",
    "ds2['DAY'] = ds2['TIME'].dt.date\n",
    "\n",
    "ds['TIME'] = pd.to_datetime(ds['TIME'])\n",
    "# Round the TIME column to the nearest hour\n",
    "ds['TIME'] = ds['TIME'].dt.round('H')\n",
    "\n",
    "ds['DAY'] = ds['TIME'].dt.date\n",
    "\"\"\"\n",
    "# Test-data - sjekk om det funker \n",
    "ds2=ds2.head(50)\n",
    "# Test-data\n",
    "ds=ds.head(50)\n",
    "print(ds.head(5))\n",
    "print(\" ds2 \")\n",
    "print(ds2.head(5))#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84aa845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a function to extract data for each group\n",
    "def extract_data_for_group(group,dsG,var='temp'):\n",
    "    date = group['DAY'].iloc[0]  # All rows in the group have the same date\n",
    "    year = date.strftime('%Y')\n",
    "    month = date.strftime('%m')\n",
    "    day = date.strftime('%d')\n",
    "    \n",
    "    file_path = f'https://thredds.met.no/thredds/dodsC/sea_norshelf_files/{year}/{month}/norshelf_qck_an_{year}{month}{day}T00Z.nc'\n",
    "    # Read the file and extract data (assuming file has some structured data)\n",
    "    \n",
    "    #extracted_data = []\n",
    "    \n",
    "    try:\n",
    "        with xr.open_dataset(file_path) as ds:\n",
    "            # Selcect variables: \n",
    "            ds = ds.get([var])\n",
    "            # Here we simulate extracting relevant data from the file for each x, y\n",
    "            extracted_data = []\n",
    "            for _, row in group.iterrows():\n",
    "                x, y = row['X'], row['Y']\n",
    "                \n",
    "                #fine time-index\n",
    "                r_time = row['TIME']\n",
    "                ocean_time = ds.ocean_time.values\n",
    "                index = find_time_index(r_time,ocean_time)\n",
    "                \n",
    "                #find depth index to interpolate over:\n",
    "                depth = row['DEPTH']\n",
    "                indices = get_Zindices(dsG,x,y,depth*-1)\n",
    "                \n",
    "                # Data extraction based on x, y, time and s_rho\n",
    "                # OBS: This is just and example of extraction, have to find the correct ocean_time and s_rho!!!!\n",
    "                temp1 = ds.isel(ocean_time=index,s_rho = indices[0], xi_rho = x, eta_rho = y)[var].values\n",
    "                temp2 = ds.isel(ocean_time=index,s_rho = indices[1], xi_rho = x, eta_rho = y)[var].values\n",
    "                m = np.array([temp1,temp2])\n",
    "                extracted_data.append(np.mean(m))\n",
    "            #return extracted_data\n",
    "    except:\n",
    "        extracted_data = []\n",
    "        for _, row in group.iterrows():\n",
    "            extracted_data.append(np.nan)\n",
    "            \n",
    "    return extracted_data   \n",
    "        \n",
    "\n",
    "        \n",
    "gridfile = '/lustre/storeB/project/fou/hi/oper/norshelf/static_inputfiles/norshelf_2.4_vert_grd.nc' #denne er fast\n",
    "# Load your ROMS dataset\n",
    "dsG = xr.open_dataset(gridfile)\n",
    "# Initialize the ROMS dataset and create the grid object\n",
    "dsG, xgrid = xroms.roms_dataset(dsG, include_cell_volume=True, include_Z0=True)\n",
    "# Associate the dataset with the grid\n",
    "dsG.xroms.set_grid(xgrid)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2950cf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ferdiiiii!!\n"
     ]
    }
   ],
   "source": [
    "# Group by the TIME column and apply the function to each group\n",
    "grouped = ds2.groupby('DAY').apply(lambda group: extract_data_for_group(group,dsG,'temp'))\n",
    "print(\"Ferdiiiii!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b701d0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ferdiiii! :P\n"
     ]
    }
   ],
   "source": [
    "# Flatten the grouped data into the original DataFrame\n",
    "ds2['TEMP_MOD'] = [item for sublist in grouped for item in sublist]\n",
    "print(\"ferdiiii! :P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6298885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ferdiiii! :P\n"
     ]
    }
   ],
   "source": [
    "# Group by the TIME column and apply the function to each group\n",
    "grouped = ds2.groupby('DAY').apply(lambda group: extract_data_for_group(group,dsG,'salt'))\n",
    "print(\"ferdiiii! :P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ecd48b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ferdiiii! :P\n"
     ]
    }
   ],
   "source": [
    "# Flatten the grouped data into the original DataFrame\n",
    "ds2['SALT_MOD'] = [item for sublist in grouped for item in sublist]\n",
    "print(\"ferdiiii! :P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dc9e344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Group by the TIME column and apply the function to each group\\ngrouped = ds2.groupby('DAY').apply(lambda group: extract_data_for_group(group,dsG,'temp'))\\n\\n    # Flatten the grouped data into the original DataFrame\\nds2['TEMP_MOD'] = [item for sublist in grouped for item in sublist]\\n\\n# Group by the TIME column and apply the function to each group\\ngrouped = ds2.groupby('DAY').apply(lambda group: extract_data_for_group(group,dsG,'salt'))\\n\\n    # Flatten the grouped data into the original DataFrame\\nds2['SALT_MOD'] = [item for sublist in grouped for item in sublist]\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Group by the TIME column and apply the function to each group\n",
    "grouped = ds2.groupby('DAY').apply(lambda group: extract_data_for_group(group,dsG,'temp'))\n",
    "\n",
    "    # Flatten the grouped data into the original DataFrame\n",
    "ds2['TEMP_MOD'] = [item for sublist in grouped for item in sublist]\n",
    "\n",
    "# Group by the TIME column and apply the function to each group\n",
    "grouped = ds2.groupby('DAY').apply(lambda group: extract_data_for_group(group,dsG,'salt'))\n",
    "\n",
    "    # Flatten the grouped data into the original DataFrame\n",
    "ds2['SALT_MOD'] = [item for sublist in grouped for item in sublist]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "610674c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hen do the same for ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dcd3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the TIME column and apply the function to each group\n",
    "grouped = ds.groupby('DAY').apply(lambda group: extract_data_for_group(group,dsG,'temp'))\n",
    "\n",
    "    # Flatten the grouped data into the original DataFrame\n",
    "ds['TEMP_MOD'] = [item for sublist in grouped for item in sublist]\n",
    "\n",
    "# Group by the TIME column and apply the function to each group\n",
    "grouped = ds.groupby('DAY').apply(lambda group: extract_data_for_group(group,dsG,'salt'))\n",
    "\n",
    "    # Flatten the grouped data into the original DataFrame\n",
    "ds['SALT_MOD'] = [item for sublist in grouped for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adc77707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save csv file\n",
    "ds.to_csv('StationM_500m_Dep2_norshelf.txt', float_format='%.3f', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aecf536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEMP</th>\n",
       "      <th>TEMP_QC</th>\n",
       "      <th>PSAL</th>\n",
       "      <th>PSAL_QC</th>\n",
       "      <th>TIME</th>\n",
       "      <th>DEPTH</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>DAY</th>\n",
       "      <th>temp</th>\n",
       "      <th>salt</th>\n",
       "      <th>TEMP_MOD</th>\n",
       "      <th>SALT_MOD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.4358</td>\n",
       "      <td>1</td>\n",
       "      <td>34.911854</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-11-27 12:00:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>66.015</td>\n",
       "      <td>1.983</td>\n",
       "      <td>406</td>\n",
       "      <td>340</td>\n",
       "      <td>2021-11-27</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>34.9175</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>34.9175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.4623</td>\n",
       "      <td>1</td>\n",
       "      <td>34.912240</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-11-27 13:00:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>66.015</td>\n",
       "      <td>1.983</td>\n",
       "      <td>406</td>\n",
       "      <td>340</td>\n",
       "      <td>2021-11-27</td>\n",
       "      <td>0.795500</td>\n",
       "      <td>34.9175</td>\n",
       "      <td>0.795500</td>\n",
       "      <td>34.9175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.4573</td>\n",
       "      <td>1</td>\n",
       "      <td>34.910988</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-11-27 14:00:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>66.015</td>\n",
       "      <td>1.983</td>\n",
       "      <td>406</td>\n",
       "      <td>340</td>\n",
       "      <td>2021-11-27</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>34.9170</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>34.9170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.4505</td>\n",
       "      <td>1</td>\n",
       "      <td>34.910341</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-11-27 15:00:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>66.015</td>\n",
       "      <td>1.983</td>\n",
       "      <td>406</td>\n",
       "      <td>340</td>\n",
       "      <td>2021-11-27</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>34.9170</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>34.9170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.4569</td>\n",
       "      <td>1</td>\n",
       "      <td>34.910742</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-11-27 16:00:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>66.015</td>\n",
       "      <td>1.983</td>\n",
       "      <td>406</td>\n",
       "      <td>340</td>\n",
       "      <td>2021-11-27</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>34.9170</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>34.9170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.4477</td>\n",
       "      <td>1</td>\n",
       "      <td>34.910972</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-11-27 17:00:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>66.015</td>\n",
       "      <td>1.983</td>\n",
       "      <td>406</td>\n",
       "      <td>340</td>\n",
       "      <td>2021-11-27</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>34.9170</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>34.9170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.4445</td>\n",
       "      <td>1</td>\n",
       "      <td>34.912828</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-11-27 18:00:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>66.015</td>\n",
       "      <td>1.983</td>\n",
       "      <td>406</td>\n",
       "      <td>340</td>\n",
       "      <td>2021-11-27</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>34.9170</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>34.9170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.4573</td>\n",
       "      <td>1</td>\n",
       "      <td>34.910747</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-11-27 19:00:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>66.015</td>\n",
       "      <td>1.983</td>\n",
       "      <td>406</td>\n",
       "      <td>340</td>\n",
       "      <td>2021-11-27</td>\n",
       "      <td>0.784500</td>\n",
       "      <td>34.9170</td>\n",
       "      <td>0.784500</td>\n",
       "      <td>34.9170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.4678</td>\n",
       "      <td>1</td>\n",
       "      <td>34.910478</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-11-27 20:00:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>66.015</td>\n",
       "      <td>1.983</td>\n",
       "      <td>406</td>\n",
       "      <td>340</td>\n",
       "      <td>2021-11-27</td>\n",
       "      <td>0.779500</td>\n",
       "      <td>34.9170</td>\n",
       "      <td>0.779500</td>\n",
       "      <td>34.9170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.4424</td>\n",
       "      <td>1</td>\n",
       "      <td>34.909580</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-11-27 21:00:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>66.015</td>\n",
       "      <td>1.983</td>\n",
       "      <td>406</td>\n",
       "      <td>340</td>\n",
       "      <td>2021-11-27</td>\n",
       "      <td>0.775499</td>\n",
       "      <td>34.9170</td>\n",
       "      <td>0.775499</td>\n",
       "      <td>34.9170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TEMP  TEMP_QC       PSAL  PSAL_QC                TIME  DEPTH     LAT  \\\n",
       "0 -0.4358        1  34.911854        1 2021-11-27 12:00:00   1000  66.015   \n",
       "1 -0.4623        1  34.912240        1 2021-11-27 13:00:00   1000  66.015   \n",
       "2 -0.4573        1  34.910988        1 2021-11-27 14:00:00   1000  66.015   \n",
       "3 -0.4505        1  34.910341        1 2021-11-27 15:00:00   1000  66.015   \n",
       "4 -0.4569        1  34.910742        1 2021-11-27 16:00:00   1000  66.015   \n",
       "5 -0.4477        1  34.910972        1 2021-11-27 17:00:00   1000  66.015   \n",
       "6 -0.4445        1  34.912828        1 2021-11-27 18:00:00   1000  66.015   \n",
       "7 -0.4573        1  34.910747        1 2021-11-27 19:00:00   1000  66.015   \n",
       "8 -0.4678        1  34.910478        1 2021-11-27 20:00:00   1000  66.015   \n",
       "9 -0.4424        1  34.909580        1 2021-11-27 21:00:00   1000  66.015   \n",
       "\n",
       "     LON    X    Y         DAY      temp     salt  TEMP_MOD  SALT_MOD  \n",
       "0  1.983  406  340  2021-11-27  0.800000  34.9175  0.800000   34.9175  \n",
       "1  1.983  406  340  2021-11-27  0.795500  34.9175  0.795500   34.9175  \n",
       "2  1.983  406  340  2021-11-27  0.790500  34.9170  0.790500   34.9170  \n",
       "3  1.983  406  340  2021-11-27  0.788000  34.9170  0.788000   34.9170  \n",
       "4  1.983  406  340  2021-11-27  0.788000  34.9170  0.788000   34.9170  \n",
       "5  1.983  406  340  2021-11-27  0.788000  34.9170  0.788000   34.9170  \n",
       "6  1.983  406  340  2021-11-27  0.787000  34.9170  0.787000   34.9170  \n",
       "7  1.983  406  340  2021-11-27  0.784500  34.9170  0.784500   34.9170  \n",
       "8  1.983  406  340  2021-11-27  0.779500  34.9170  0.779500   34.9170  \n",
       "9  1.983  406  340  2021-11-27  0.775499  34.9170  0.775499   34.9170  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save csv file\n",
    "ds2.to_csv('StationM_Dep1_norshelf.txt', float_format='%.3f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bbe06a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
