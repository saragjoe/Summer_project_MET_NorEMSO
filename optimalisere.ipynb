{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd053509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import xarray as xr\n",
    "import numpy as np \n",
    "import xroms\n",
    "import pyresample \n",
    "import metpy\n",
    "import matplotlib.pyplot as plt\n",
    "import os, glob\n",
    "\n",
    "#FUNCTIONS\n",
    "\n",
    "# TEXTFILE TO PANDAS\n",
    "def txt_to_pd(txtfile,LAT,LON):\n",
    "\n",
    "    columns_to_keep = ['T_degC', 'T_qf', 'S', 'S_qf', 'Date', 'Time'] \n",
    "    new_column_names = ['TEMP','TEMP_QC','PSAL','PSAL_QC','Date','Time']\n",
    "\n",
    "\n",
    "    # Hvis vi vil at pandas skal tolke dato som som et datetime object \n",
    "    # kan vi gi informasjon om hvordan dato stirngene er formatert med parser.  \n",
    "    #\n",
    "    # For denne filen vil det kunne se slik ut:\n",
    "    # parse = lambda x: datetime.datetime.strptime(x, '%d %b %Y %H:%M:%S')\n",
    "    # Betydningen av de ulike %bokstaven finnes her: (https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior)\n",
    "\n",
    "    # Men for å gjøre ting litt vanskligere er det brukt norske forkortelser for månedsnavn... \n",
    "    # Vi bytter enkelt ut norske forkortelser med engelske ved å bruke metoden .replace(norsk, engelsk)\n",
    "\n",
    "    parse = lambda x: datetime.datetime.strptime(x.replace('Des', 'Dec').replace('Mai', 'May').replace('Okt', 'Oct'), '%d %b %Y %H:%M:%S')\n",
    "\n",
    "    #df = pd.read_csv(txtfile, delimiter='\\t', usecols=columns_to_keep, parse_dates={\"Datetime\" : ['Date', 'Time']}, date_parser = parse)\n",
    "    df = pd.read_csv(txtfile, delimiter='\\t', usecols=columns_to_keep)\n",
    "    df.columns = new_column_names\n",
    "    \n",
    "    # Combine Date and Time columns and strip any leading or trailing whitespace\n",
    "    df['TIME'] = (df['Date'] + ' ' + df['Time']).str.strip()\n",
    "    \n",
    "    # Apply the custom parsing function\n",
    "    df['TIME'] = df['TIME'].apply(parse)\n",
    "    df.drop(columns=['Date', 'Time'], inplace=True)\n",
    "    \n",
    "    # Add depth:\n",
    "    df['DEPTH'] = get_depth(txtfile)\n",
    "    df['LAT'] = LAT\n",
    "    df['LON'] = LON\n",
    "    return(df)\n",
    "\n",
    "#NETCDF FILE TO PANDAS:\n",
    "def netcdf_to_pd(txt):\n",
    "    # Load the NetCDF file using xarray\n",
    "    ds = xr.open_dataset(txt)\n",
    "\n",
    "    # Select the variables you are interested in\n",
    "    variables = ['TEMP', 'TEMP_QC', 'PSAL', 'PSAL_QC']\n",
    "\n",
    "    # Initialize an empty DataFrame to merge into\n",
    "    df_combined = pd.DataFrame()\n",
    "\n",
    "    # Loop over each variable to process and merge\n",
    "    for var in variables:\n",
    "\n",
    "        # Select the variable data\n",
    "        data = ds[var]\n",
    "    \n",
    "        # Stack the depth and time dimensions into a MultiIndex\n",
    "        stacked_data = data.stack(points=('DEPTH', 'TIME'))\n",
    "    \n",
    "    \n",
    "        # Convert the stacked DataArray to a pandas DataFrame\n",
    "        df = stacked_data.to_dataframe()\n",
    "            \n",
    "        # Temporarily rename columns to avoid conflicts\n",
    "        df.rename(columns={'DEPTH': 'Depth_col', 'TIME': 'Time_col', 'LONGITUDE': 'LON_col'}, inplace=True)\n",
    "        #        \n",
    "        df = df.reset_index()\n",
    "        df.drop(columns=['Depth_col', 'Time_col'], inplace=True)\n",
    "        #    \n",
    "        # If df_combined is empty, initialize it with the current DataFrame\n",
    "        if df_combined.empty:\n",
    "            df_combined = df\n",
    "        else:\n",
    "            # Merge the current DataFrame with the combined DataFrame\n",
    "            df_combined = pd.merge(df_combined, df, on=['DEPTH', 'TIME'], how='outer')  \n",
    "    df_combined['LON'] = ds['LONGITUDE'].values[0]\n",
    "    df_combined['LAT'] = ds['LATITUDE'].values[0]\n",
    "    return(df_combined)     \n",
    "\n",
    "# GET DEPTH FROM FILENAME\n",
    "def get_depth(filename):\n",
    "    # Use a regular expression to extract the depth value\n",
    "    depth_match = re.search(r'_(\\d+)m', filename)\n",
    "    if depth_match:\n",
    "        depth_value = int(depth_match.group(1))\n",
    "    else:\n",
    "        depth_value = None\n",
    "    return(depth_value)   \n",
    "\n",
    "# FIND DEPTH INDEX IN ROMS (weighted mean)\n",
    "def get_Zindices(ds,X,Y,target_depth):\n",
    "    ds = ds.isel(xi_rho = X, eta_rho = Y)\n",
    "\n",
    "    # Extract the z_rho values at the given point\n",
    "    z_rho_values = ds.z_rho.values\n",
    "    s_rho_values = ds.s_rho.values\n",
    "\n",
    "    diff = np.abs(z_rho_values - target_depth)\n",
    "    diff = diff.squeeze()\n",
    "    \n",
    "    # Find the indices of the two smallest differences\n",
    "    closest_indices = diff.argsort()[:2]\n",
    "    \n",
    "    # Calculate the weights as the inverse of the differences\n",
    "    # Adding a small epsilon to avoid division by zero\n",
    "    epsilon = 1e-10\n",
    "    weights = 1 / (diff[closest_indices] + epsilon)\n",
    "\n",
    "    # Normalize the weights so that they sum to 1\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    return(weights,closest_indices)\n",
    "\n",
    "# FIND TIME-INDEX IN ROMS RELATIVE TO OBSERVATION TIME\n",
    "def find_time_index(r_time,ocean_time):\n",
    "    days_since_1970 = (r_time - datetime.datetime(1970,1,1,0,0,0)).total_seconds()\n",
    "    ocean_time = pd.to_datetime(ocean_time)\n",
    "    # Convert ocean_time\n",
    "    ocean_time_since_1970 = (ocean_time - datetime.datetime(1970,1,1,0,0,0)).total_seconds()\n",
    "    index = np.abs(ocean_time_since_1970 - days_since_1970).argmin()\n",
    "    return(index)\n",
    "\n",
    "# FIND THE ROMS-MODEL's X AND Y POSITIONS\n",
    "def get_XYpositions(filename, lons, lats):\n",
    "    \n",
    "    fh = xr.open_dataset(filename)\n",
    "    x   = np.linspace(0, fh.lat_rho.values.shape[1]-1, fh.lat_rho.values.shape[1])\n",
    "    y   = np.linspace(0, fh.lat_rho.values.shape[0]-1, fh.lat_rho.values.shape[0])\n",
    "    xi  = np.zeros_like(fh.lon_rho.values)\n",
    "    yi  = np.zeros([fh.lon_rho.values.shape[1], fh.lon_rho.values.shape[0]])\n",
    "    xi[:,:] = x\n",
    "    yi[:,:] = y\n",
    "    yi  = np.swapaxes(yi, 1, 0)\n",
    "\n",
    "    # First I define the wet points of the field as the lon,lat values with mask_rho==1 \n",
    "    sea_def = pyresample.geometry.SwathDefinition(lons= fh.lon_rho.values[np.where(fh.mask_rho)], lats = fh.lat_rho.values[np.where(fh.mask_rho)])\n",
    "\n",
    "    # Second, the full grid definiton (our target domain):\n",
    "    orig_def = pyresample.geometry.SwathDefinition(lons=lons, lats=lats)\n",
    "\n",
    "    # Then I fill the temperature field by the nearest neighbour approace.\n",
    "    # Note that only wet points are used as input. \n",
    "\n",
    "    # The radius of influence sets a limit (in meters) for how far away a true value can be from the point that will be filled\n",
    "\n",
    "    ypos = pyresample.kd_tree.resample_nearest(sea_def, yi[np.where(fh.mask_rho)], \\\n",
    "                               orig_def, radius_of_influence=2400)\n",
    "\n",
    "    xpos = pyresample.kd_tree.resample_nearest(sea_def, xi[np.where(fh.mask_rho)], \\\n",
    "                               orig_def, radius_of_influence=2400)\n",
    "    return np.array([int(x) for x in xpos]), np.array([int(y) for y in ypos])\n",
    "\n",
    "\n",
    "# MAIN FUNCTION THAT EXTRACTS DATA FROM THE ROMS MODEL, AS LONG AS THE PANDAS DATAFRAME IS GROUPED INTO DAYS\n",
    "def extract_data_for_group(group,dsG,var='temp'):\n",
    "    # Define a function to extract data for each group:\n",
    "    date = group['DAY'].iloc[0]  # All rows in the group have the same date\n",
    "    year = date.strftime('%Y')\n",
    "    month = date.strftime('%m')\n",
    "    day = date.strftime('%d')\n",
    "    \n",
    "    file_path = f'https://thredds.met.no/thredds/dodsC/sea_norshelf_files/{year}/{month}/norshelf_qck_an_{year}{month}{day}T00Z.nc'\n",
    "                  \n",
    "    # Read the file and extract data (assuming file has some structured data)\n",
    "    \n",
    "    try:\n",
    "        with xr.open_dataset(file_path) as ds:\n",
    "            # Selcect variables: \n",
    "            ds = ds.get([var])\n",
    "            # Here we simulate extracting relevant data from the file for each x, y\n",
    "            extracted_data = []\n",
    "            for _, row in group.iterrows():\n",
    "                x, y = row['X'], row['Y']\n",
    "                \n",
    "                #fine time-index\n",
    "                r_time = row['TIME']\n",
    "                ocean_time = ds.ocean_time.values\n",
    "                index = find_time_index(r_time,ocean_time)\n",
    "                \n",
    "                #find depth index to interpolate over:\n",
    "                depth = row['DEPTH']\n",
    "                weights,indices = get_Zindices(dsG,x,y,depth*-1)\n",
    "                \n",
    "                # Data extraction based on x, y, time and s_rho\n",
    "                temp1 = ds.isel(ocean_time=index,s_rho = indices[0], xi_rho = x, eta_rho = y)[var].values\n",
    "                temp2 = ds.isel(ocean_time=index,s_rho = indices[1], xi_rho = x, eta_rho = y)[var].values\n",
    "                # Calculate the weighted average temperature\n",
    "                weighted_temperature = temp1 * weights[0] + temp2 * weights[1]\n",
    "                extracted_data.append(weighted_temperature)\n",
    "            return extracted_data\n",
    "    except:\n",
    "        extracted_data = []\n",
    "        for _, row in group.iterrows():\n",
    "            extracted_data.append(np.nan)\n",
    "        return extracted_data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43590021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILES:\n",
    "# Grid file Norshelf:\n",
    "gridfile = '/lustre/storeB/project/fou/hi/oper/norshelf/static_inputfiles/norshelf_2.4_vert_grd.nc'\n",
    "\n",
    "# ADD INFORMATION TO GRIDFILE TO FIND DEPTH-index:\n",
    "\n",
    "# Load your ROMS dataset\n",
    "dsG = xr.open_dataset(gridfile)\n",
    "# Initialize the ROMS dataset and create the grid object\n",
    "dsG, xgrid = xroms.roms_dataset(dsG, include_cell_volume=True, include_Z0=True)\n",
    "# Associate the dataset with the grid\n",
    "dsG.xroms.set_grid(xgrid)\n",
    "\n",
    "\n",
    "#StationM (txt)\n",
    "INfile = \"/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/SouthCape/T_S_SouthCape.csv\"\n",
    "#'/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment3/StaM_SBE_20221125_2000m.txt'\n",
    "\n",
    "#StationM (netCDF):\n",
    "#INfile = '/lustre/storeB/project/fou/hi/projects/NorEmso/Observations/moorings/StationM/Deployment1/StationM_2021_hydrography.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f74483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/modules/rhel8/conda/install/envs/production-10-2022/lib/python3.9/site-packages/pandas/core/arraylike.py:405: RuntimeWarning: invalid value encountered in z_from_p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'lon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m ds \u001b[38;5;241m=\u001b[39m csv_to_pd(INfile,\u001b[38;5;241m76.107\u001b[39m, \u001b[38;5;241m15.967\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# READING NETCDF FILES:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#ds = netcdf_to_pd(INfile)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# MOVE THIS INTO A FUNCTION AND RE-USE THEM IN THE FUNCTIONS txt_to_pd and netcdf_to_pd\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m x,y \u001b[38;5;241m=\u001b[39m get_XYpositions(gridfile, ds\u001b[38;5;241m.\u001b[39mlon\u001b[38;5;241m.\u001b[39mvalues, ds\u001b[38;5;241m.\u001b[39mlat\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     12\u001b[0m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     13\u001b[0m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y\n",
      "File \u001b[0;32m/modules/rhel8/conda/install/envs/production-10-2022/lib/python3.9/site-packages/pandas/core/generic.py:5907\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5901\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5902\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5903\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5904\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5905\u001b[0m ):\n\u001b[1;32m   5906\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'lon'"
     ]
    }
   ],
   "source": [
    "# FIND X and Y:\n",
    "\n",
    "# READING TEXT FILES\n",
    "ds = txt_to_pd(INfile,66.015,1.983)\n",
    "\n",
    "# READING NETCDF FILES:\n",
    "#ds = netcdf_to_pd(INfile)\n",
    "\n",
    "# MOVE THIS INTO A FUNCTION AND RE-USE THEM IN THE FUNCTIONS txt_to_pd and netcdf_to_pd\n",
    "\n",
    "x,y = get_XYpositions(gridfile, ds.LON.values, ds.LAT.values)\n",
    "ds['X'] = x\n",
    "ds['Y'] = y\n",
    "\n",
    "\n",
    "# Time to datetime:\n",
    "ds['TIME'] = pd.to_datetime(ds['TIME'])\n",
    "\n",
    "# Round the TIME column to the nearest hour:\n",
    "ds['TIME'] = ds['TIME'].dt.round('H')\n",
    "\n",
    "# Round the TIME column to the nearest day:\n",
    "ds['DAY'] = ds['TIME'].dt.date\n",
    "\n",
    "# Extract unique months (Year-Month) from the date column\n",
    "ds['year_month'] = ds['TIME'].dt.to_period('M')\n",
    "\n",
    "# Test-data\n",
    "#ds=ds.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET MODELRESULTS:\n",
    "\n",
    "# Find unique Year-Month combinations\n",
    "unique_months = ds['year_month'].unique()\n",
    "\n",
    "for month in unique_months:\n",
    "    monthly_data = ds.loc[ds['year_month'] == month].copy() \n",
    "    print(str(month))\n",
    "    # Group by the TIME column and apply the function to each group\n",
    "    grouped = monthly_data.groupby('DAY').apply(lambda group: extract_data_for_group(group,dsG,'temp'))\n",
    "    # Flatten the grouped data into the original DataFrame\n",
    "    monthly_data['TEMP_MOD'] = [item for sublist in grouped for item in sublist]\n",
    "    #!!!! DO the same for salt\n",
    "    # Group by the TIME column and apply the function to each group\n",
    "    grouped = monthly_data.groupby('DAY').apply(lambda group: extract_data_for_group(group,dsG,'salt'))\n",
    "    # Flatten the grouped data into the original DataFrame\n",
    "    monthly_data['SALT_MOD'] = [item for sublist in grouped for item in sublist]\n",
    "    #save_file:\n",
    "    filename = INfile.split('.')[0].split('/')[-1]+'_{}.csv'\n",
    "    monthly_data.to_csv(filename.format(str(month)), float_format='%.3f', index=False)\n",
    "    del monthly_data\n",
    "\n",
    "dsG.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe2741c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dae5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
